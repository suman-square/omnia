# Copyright 2024 Dell Inc. or its subsidiaries. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
- name: Update Inventory with ansible_host information
  ansible.builtin.import_playbook: ../utils/servicetag_host_mapping.yml
  when: not ( hostvars['127.0.0.1']['update_inventory_executed'] | default(false) | bool )

- name: Gather facts from all the nodes
  hosts: slurm_control_node, kube_control_plane, slurm_node, kube_node, login, etcd

- name: Include input project directory
  ansible.builtin.import_playbook: ../utils/include_input_dir.yml

- name: Create kubespray container group
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_kubespray_group: true

- name: Validate scheduler input parameters
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Validate scheduler input parameters
      ansible.builtin.include_role:
        name: cluster_validation
        tasks_from: validation_status_check.yml

# - name: Update Repositories/Registries on nodes
#   ansible.builtin.import_playbook: ../utils/update_user_repo.yml
#   when: not ( hostvars['127.0.0.1']['update_user_repo_executed'] | default(false) | bool )

- name: Apply common Slurm installation and config
  hosts: slurm_control_node, slurm_node, login
  gather_facts: false
  any_errors_fatal: true
  roles:
    - slurm_common

- name: Prepare kube control plane for kubernetes installations
  hosts: kube_control_plane
  gather_facts: false
  roles:
    - k8s_manager

- name: Prepare kube control plane, kube nodes and etcd for kubernetes installations
  hosts: kube_control_plane, kube_node, etcd
  gather_facts: false
  any_errors_fatal: true
  roles:
    - k8s_prepare_nodes

- name: Start Kubernetes Installations
  hosts: omnia_kubespray
  vars:
    k8s_nfs_share: "/opt/omnia/kubespray"
    k8s_logs_dir: "/opt/omnia/log/kubespray"
    dir_mode: '0755'
  tasks:
    - name: Copy k8s_start_setup to kubespray nfs share
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/playbooks/k8s_start_setup.yml"
        dest: "{{ k8s_nfs_share }}/k8s_start_setup.yml"
    
    - name: Ensure /log/kubespray directory exists
      ansible.builtin.file:
        path: "{{ k8s_logs_dir }}"
        state: directory
        mode: "{{ dir_mode }}"

    - name: Execute ansible-playbook for Kubernetes setup asynchronously
      ansible.builtin.shell:
        cmd: /venv/bin/ansible-playbook {{ k8s_nfs_share }}/k8s_start_setup.yml -i {{ k8s_nfs_share }}/inv_k8s --extra-vars "@{{ k8s_nfs_share }}/k8s_all_vars.yml" -vvv | /usr/bin/tee {{ k8s_logs_dir }}//k8s_deployment.log
      async: 3600  # Set async timeout (e.g., 1 hour)
      poll: 0  # Non-blocking (continue the playbook without waiting for completion)
      register: result  # Register the result to capture job ID

    - name: Wait for the Kubernetes installation to finish. Logs can be checked at /opt/omnia/log/kubespray/k8s_deployment.log
      ansible.builtin.async_status:
        jid: "{{ result.ansible_job_id }}"  # Job ID from the previous task
      register: job_result
      until: job_result.finished
      retries: 60  # Retry the task 60 times (1 hour total)
      delay: 10  # Wait 10 seconds between retries

- name: Add nodes to kubernetes cluster
  ansible.builtin.import_playbook: "{{ playbook_dir }}/playbooks/k8s_add_node.yml"

- name: Prepare habana container runtime
  hosts: kube_node
  gather_facts: false
  roles:
    - k8s_habana_container_runtime

- name: Install nvidia-container-toolkit on nodes with Nvidia GPU
  hosts: kube_control_plane, kube_node, etcd
  gather_facts: false
  roles:
    - k8s_nvidia_container_toolkit

- name: Install Plugin only when one of nodes with AMD GPU
  hosts: kube_control_plane, kube_node, etcd
  gather_facts: false
  roles:
    - k8s_amd

- name: Prepare kube control plane and kube nodes for kubernetes services installations
  hosts: kube_control_plane, kube_node
  gather_facts: false
  roles:
    - k8s_prepare_services

- name: Start K8s worker servers on kube control nodes
  hosts: kube_control_plane
  gather_facts: false
  roles:
    - k8s_start_services

- name: CSI powerscale image pulling
  hosts: kube_node, kube_control_plane
  tasks:
    - name: Pull images
      ansible.builtin.include_role:
        name: k8s_csi_powerscale_plugin
        tasks_from: csi_powerscale_image_pull.yml
      when: hostvars['127.0.0.1']['csi_driver_powerscale_precheck_pass'] | default(false) | bool

- name: Install CSI powerscale plugin on kube control nodes
  hosts: kube_control_plane
  gather_facts: false
  roles:
    - k8s_csi_powerscale_plugin

- name: Apply slurm control node config
  hosts: slurm_control_node
  gather_facts: false
  roles:
    - slurm_manager

- name: Configure Slurm worker nodes
  hosts: slurm_node, login
  gather_facts: false
  roles:
    - slurm_workers

- name: Start slurm services on slurm control node
  hosts: slurm_control_node
  gather_facts: false
  roles:
    - slurm_start_services

- name: Start slurm services on slurm node and login
  hosts: slurm_node, login
  gather_facts: false
  roles:
    - slurm_workers_service

- name: Setup slurm pam authentication
  hosts: slurm_control_node, slurm_node, login
  gather_facts: true
  roles:
    - slurm_pam

- name: Compile and install the ucx and openmpi on the nfs share of compute nodes
  hosts: slurm_control_node, kube_control_plane
  gather_facts: true
  roles:
    - install_benchmarks_tools
