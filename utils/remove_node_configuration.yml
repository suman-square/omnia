#  Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

- name: Include input project directory
  ansible.builtin.import_playbook: ../utils/include_input_dir.yml
  tags: always

- name: Create kubespray container group
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_kubespray_group: true
    omnia_provision_group: true
  tags: always

- name: Warning and User confirmation for removing slurm and kube node
  hosts: localhost
  connection: local
  gather_facts: false
  roles:
    - { role: remove_node/user_confirmation } # noqa: role-name[path]
    - { role: common }
  tags: always

- name: Host Mapping
  hosts: omnia_provision
  connection: ssh
  roles:
    - servicetag_host_mapping
  tags: always

- name: Proceeding to remove slurm login node
  tags: login_node
  hosts: localhost
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/validate_slurm_node } # noqa: role-name[path]

- name: Remove login node
  tags: login_node
  hosts: remove_login_node
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/remove_slurm_node } # noqa: role-name[path]

- name: Proceeding to remove slurm node
  tags: slurm_node
  hosts: localhost
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_node/validate_slurm_node } # noqa: role-name[path]

- name: Remove slurm node
  tags: slurm_node
  hosts: remove_slurm_node
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/remove_slurm_node } # noqa: role-name[path]

- name: Proceeding to remove kube node
  tags: kube_node
  hosts: omnia_provision
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_node/remove_kube_node } # noqa: role-name[path]
  tasks:
    - name: Add host
      when: removing_nodes is defined and removing_nodes | length > 0
      ansible.builtin.add_host:
        name: "remove_nodes"
        nodes_removing: "{{ removing_nodes }}"

- name: Remove kube nodes
  hosts: omnia_kubespray
  tags: kube_node
  vars:
    node: "{{ hostvars['remove_nodes']['nodes_removing'] }}"
    k8s_nfs_share: "/opt/omnia/kubespray"
    k8s_log_dir: "/opt/omnia/log/kubespray"
    dir_mode: '0755'
  tasks:
    - name: Execute tasks for Remove kube nodes
      block:
        - name: Copy remove_kube_node to kubespray nfs share
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/playbooks/remove_kube_node.yml"
            dest: "{{ k8s_nfs_share }}/remove_kube_node.yml"

        - name: Ensure log/kubespray directory exists
          ansible.builtin.file:
            path: "{{ k8s_log_dir }}"
            state: directory
            mode: "{{ dir_mode }}"

        - name: Execute ansible-playbook for Kubernetes add node asynchronously
          ansible.builtin.shell:
            cmd: /venv/bin/ansible-playbook {{ k8s_nfs_share }}/remove_kube_node.yml -i {{ k8s_nfs_share }}/inv_k8s --extra-vars "@{{ k8s_nfs_share }}/k8s_all_vars.yml" --extra-vars "node={{ node }}" -vvv | /usr/bin/tee {{ k8s_log_dir }}/k8s_remove_nodes.log
          async: 3600  # Set async timeout (e.g., 1 hour)
          poll: 0  # Non-blocking (continue the playbook without waiting for completion)
          register: k8s_remove_result  # Register the result to capture job ID
        
        - name: Wait for the Kubernetes remove kube_nodes to finish. Logs can be checked at /opt/omnia/log/kubespray/k8s_remove_nodes.log
          ansible.builtin.async_status:
            jid: "{{ k8s_remove_result.ansible_job_id }}"  # Job ID from the previous task
          register: job_result
          until: job_result.finished
          retries: 60  # Retry the task 60 times (1 hour total)
          delay: 10  # Wait 10 seconds between retries
      when: hostvars['remove_nodes']['nodes_removing'] is defined

# manually removing kube service address from /etc/resolv.conf
- name: Remove kube service address from kube nodes
  tags: kube_node
  hosts: remove_kube_node
  connection: ssh
  roles:
    - { role: remove_node/post_kube_node_removal } # noqa: role-name[path]

- name: Confirm slurm_node removal
  hosts: remove_slurm_node
  connection: ssh
  gather_facts: false
  tags: slurm_node
  roles:
    - { role: remove_node/verify_slurm_node_removal } # noqa: role-name[path]

- name: Confirm kube_node removal
  hosts: remove_kube_node
  connection: ssh
  gather_facts: false
  tags: kube_node
  roles:
    - { role: remove_node/verify_kube_node_removal } # noqa: role-name[path]
